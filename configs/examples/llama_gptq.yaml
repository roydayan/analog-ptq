# Example configuration for quantizing LLaMA-3.2-1B-Instruct with GPTQ
# 
# Usage:
#   analog-ptq configs/examples/llama_gptq.yaml
#   # or
#   python scripts/run_experiment.py configs/examples/llama_gptq.yaml

experiment:
  name: "llama3.2-1b-instruct-gptq-4bit"
  output_dir: "./outputs/llama3.2-1b-instruct-gptq"
  seed: 42

model:
  name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  dtype: "float16"
  device_map: "cuda:0"  # Use single GPU for GPTQ compatibility
  trust_remote_code: false

quantization:
  method: "gptq"
  bits: 4
  group_size: 128
  symmetric: false
  damp_percent: 0.01
  block_size: 128
  actorder: false
  calibration:
    dataset: "wikitext"
    num_samples: 128
    seq_length: 2048
    seed: 42

# Evaluation disabled - lm-eval has outdated dataset revisions
# To evaluate, either: (1) upgrade lm-eval in a separate venv, or (2) use other eval tools
# evaluation:
#   tasks:
#     - "winogrande"
#     - "piqa" 
#     - "boolq"
#   batch_size: 8
#   num_fewshot: 0
#   limit: 100
